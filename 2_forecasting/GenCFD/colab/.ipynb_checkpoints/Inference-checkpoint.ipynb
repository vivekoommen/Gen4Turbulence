{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5265e973-cd51-4484-a1dd-f49228909346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup relevant libraries\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "\n",
    "os.chdir(\"/oscar/data/gk/voommen/no_diffusion/hit_3d/GenCFD_lf2\")\n",
    "\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import GenCFD\n",
    "from GenCFD import diffusion as dfn_lib\n",
    "from GenCFD import model, train, solvers, utils\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "array = np.ndarray\n",
    "\n",
    "DATA_STD = 0.5 # Fixed parameter but can also be learnable\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 0\n",
    "\n",
    "# Setting global seed for reproducibility\n",
    "torch.manual_seed(SEED)  # For CPU operations\n",
    "torch.cuda.manual_seed(SEED)  # For GPU operations\n",
    "torch.cuda.manual_seed_all(SEED)  # Ensure all GPUs (if multi-GPU) are set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e3ec79-6cbe-41e4-b3ac-55823d4b9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f0589-c257-4fdd-8950-23ce598df22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c23b607-5ac1-4219-89f7-a19847502bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "# Set the cache size and debugging for torch.compile before importing torch\n",
    "# os.environ[\"TORCH_LOGS\"] = \"all\"  # or any of the valid log settings\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.distributed import is_initialized\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from GenCFD.train import training_loop\n",
    "from GenCFD.utils.dataloader_builder import get_dataset_loader\n",
    "from GenCFD.utils.gencfd_builder import (\n",
    "    create_denoiser,\n",
    "    create_callbacks,\n",
    "    save_json_file,\n",
    ")\n",
    "from GenCFD.utils.parser_utils import train_args\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")  # Better performance on newer GPUs!\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 0\n",
    "\n",
    "# Setting global seed for reproducibility\n",
    "torch.manual_seed(SEED)  # For CPU operations\n",
    "torch.cuda.manual_seed(SEED)  # For GPU operations\n",
    "torch.cuda.manual_seed_all(SEED)  # Ensure all GPUs (if multi-GPU) are set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abff98d0-00c4-4997-918f-ee38f11ce9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_distributed_mode(args):\n",
    "    \"\"\"Initialize a Distributed Data Parallel Environment\"\"\"\n",
    "\n",
    "    args.local_rank = int(os.getenv(\"LOCAL_RANK\", -1))  # Get from environment variable\n",
    "\n",
    "    if args.local_rank == -1:\n",
    "        raise ValueError(\n",
    "            \"--local_rank was not set. Ensure torchrun is used to launch the script.\"\n",
    "        )\n",
    "\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\", rank=args.local_rank, world_size=args.world_size\n",
    "    )\n",
    "\n",
    "    device = torch.device(f\"cuda:{args.local_rank}\")\n",
    "    print(\" \")\n",
    "    print(f\"DDP initialized with rank {args.local_rank} and device {device}.\")\n",
    "\n",
    "    return args, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901590c-352d-4bed-8152-0fa3e8a5088d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "324b058b-8a16-4fc8-aa8c-7082af02a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_tuple(value):\n",
    "    \"\"\"Allows for tuples as arguments.\"\"\"\n",
    "\n",
    "    if value is None or value.lower() == \"none\":\n",
    "        return None\n",
    "    try:\n",
    "        return tuple(map(int, value.strip(\"()\").split(\",\")))\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid tuple format: {value}\")\n",
    "\n",
    "\n",
    "def str_to_bool(value):\n",
    "    \"\"\"Transform a string to a bool.\"\"\"\n",
    "\n",
    "    if value.lower() in [\"true\", \"t\", \"1\", \"yes\", \"y\"]:\n",
    "        return True\n",
    "    elif value.lower() in [\"false\", \"f\", \"0\", \"no\", \"n\"]:\n",
    "        return False\n",
    "\n",
    "    if isinstance(value, bool):  # If it's already a boolean, return as is\n",
    "        return value\n",
    "\n",
    "\n",
    "def add_base_options(parser: ArgumentParser):\n",
    "    \"\"\"General base arguments for training and inference\"\"\"\n",
    "\n",
    "    group = parser.add_argument_group(\"base\")\n",
    "    # Mixed precision calculations activate a scalar for the gradient propagation and uses float16 where possible\n",
    "    group.add_argument(\n",
    "        \"--work_dir\",\n",
    "        default=\"datasets\",\n",
    "        type=str,\n",
    "        help=\"If empty, will use defaults according to the specified dataset.\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--save_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Specify your directory where training or evaluation results should be saved\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--model_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"Set a path to a pretrained model for inference\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--dtype\",\n",
    "        default=torch.float32,\n",
    "        type=torch.dtype,\n",
    "        help=\"Set the precision for PyTorch tensors by defining the dtype\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--use_mixed_precision\",\n",
    "        default=True,\n",
    "        type=str_to_bool,\n",
    "        help=\"For memory efficiency activate mixed precision calculations\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_parallelization_options(parser: ArgumentParser):\n",
    "    \"\"\"Arguments for Distributed Training\"\"\"\n",
    "\n",
    "    group = parser.add_argument_group(\"distributed\")\n",
    "    group.add_argument(\n",
    "        \"--local_rank\", type=int, default=-1, help=\"Local rank for distributed training\"\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--world_size\", type=int, default=1, help=\"Total number of processes for DDP\"\n",
    "    )\n",
    "\n",
    "\n",
    "def add_data_options(parser: ArgumentParser):\n",
    "    \"\"\"Relevant parser arguments for the dataloader\"\"\"\n",
    "\n",
    "    group = parser.add_argument_group(\"dataset\")\n",
    "    group.add_argument(\n",
    "        \"--dataset\",\n",
    "        default=\"DataIC_Vel\",\n",
    "        type=str,\n",
    "        choices=[\n",
    "            # Datasets for Training\n",
    "            \"ShearLayer2D\",\n",
    "            \"CloudShock2D\",\n",
    "            \"RichtmyerMeshkov2D\",\n",
    "            \"ShearLayer3D\",\n",
    "            \"TaylorGreen3D\",\n",
    "            \"Nozzle3D\",\n",
    "            # Conditional (Perturbed) Datasets for Evaluation\n",
    "            \"ConditionalShearLayer2D\",\n",
    "            \"ConditionalCloudShock2D\",\n",
    "            \"ConditionalShearLayer3D\",\n",
    "            \"ConditionalTaylorGreen3D\",\n",
    "            \"ConditionalNozzle3D\",\n",
    "            \"HIT3D\"\n",
    "        ],\n",
    "        help=\"Name of the dataset, available choices\",\n",
    "    )\n",
    "    group.add_argument(\"--batch_size\", default=5, type=int, help=\"Choose a batch size\")\n",
    "    group.add_argument(\n",
    "        \"--worker\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Choose the number of worker for parallel processing\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_model_options(parser: ArgumentParser):\n",
    "    \"\"\"Relevant parser arguments for the UNet architecture\"\"\"\n",
    "\n",
    "    group = parser.add_argument_group(\"model\")\n",
    "    # Model settings\n",
    "    group.add_argument(\n",
    "        \"--model_type\",\n",
    "        default=\"PreconditionedDenoiser\",\n",
    "        type=str,\n",
    "        choices=[\n",
    "            \"PreconditionedDenoiser\",\n",
    "            \"UNet\",\n",
    "            \"PreconditionedDenoiser3D\",\n",
    "            \"UNet3D\",\n",
    "        ],\n",
    "        help=\"Choose a valid Neural Network Model architecture\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--num_channels\",\n",
    "        default=(64, 128),\n",
    "        type=parse_tuple,\n",
    "        help=\"Number of channels for down and upsampling\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--downsample_ratio\",\n",
    "        default=(2, 2),\n",
    "        type=parse_tuple,\n",
    "        help=\"Choose a downsample ratio\",\n",
    "    )\n",
    "    # Attention settings\n",
    "    group.add_argument(\n",
    "        \"--use_attention\",\n",
    "        default=True,\n",
    "        type=str_to_bool,\n",
    "        help=\"Choose if attention blocks should be used\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--num_blocks\", default=4, type=int, help=\"Choose number of Attention blocks\"\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--num_heads\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "        help=\"Choose number of heads for multihead attention\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--normalize_qk\",\n",
    "        default=False,\n",
    "        type=str_to_bool,\n",
    "        help=\"Choose if Query and Key matrix should be normalized\",\n",
    "    )\n",
    "    # Embedding settings\n",
    "    group.add_argument(\n",
    "        \"--noise_embed_dim\",\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help=\"Choose noise embedding dimension\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--use_position_encoding\",\n",
    "        default=True,\n",
    "        type=str_to_bool,\n",
    "        help=\"Use position encoding True or False\",\n",
    "    )\n",
    "    # General settings\n",
    "    group.add_argument(\n",
    "        \"--padding_method\",\n",
    "        default=\"circular\",\n",
    "        type=str,\n",
    "        choices=[\n",
    "            \"circular\",\n",
    "            \"constant\",\n",
    "            \"reflect\",\n",
    "            \"lonlat\",\n",
    "            \"latlon\",\n",
    "            \"same\",\n",
    "            \"zeros\",\n",
    "        ],\n",
    "        help=\"Choose a proper padding method from the list of choices\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--dropout_rate\", default=0.0, type=float, help=\"Choose a proper dropout rate\"\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--use_hr_residual\",\n",
    "        default=False,\n",
    "        type=str_to_bool,\n",
    "        help=\"Dropout rate for classifier-free guidance\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--sigma_data\",\n",
    "        default=0.5,\n",
    "        type=float,\n",
    "        help=\"This can be a fixed in [0, 1] or learnable parameter\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--resize_to_shape\",\n",
    "        default=None,\n",
    "        type=parse_tuple,\n",
    "        help=\"Choose a shape to resize inside the UNet. Necessary if dataset resolution changes\",\n",
    "    )\n",
    "    # Compile model setting\n",
    "    group.add_argument(\n",
    "        \"--compile\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If True, model will be compiled. This allows for faster training and inference\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_denoiser_options(parser: ArgumentParser):\n",
    "    \"\"\"Relevant parameter for the DenoisingModel\"\"\"\n",
    "\n",
    "    group = parser.add_argument_group(\"denoiser\")\n",
    "    group.add_argument(\n",
    "        \"--diffusion_scheme\",\n",
    "        default=\"create_variance_exploding\",\n",
    "        choices=[\"create_variance_preserving\", \"create_variance_exploding\"],\n",
    "        help=\"Choose a valid diffusion scheme\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--sigma\",\n",
    "        default=\"exponential_noise_schedule\",\n",
    "        choices=[\n",
    "            \"exponential_noise_schedule\",\n",
    "            \"power_noise_schedule\",\n",
    "            \"tangent_noise_schedule\",\n",
    "        ],\n",
    "        help=\"Choose a valid noise scheduler sigma\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--noise_sampling\",\n",
    "        default=\"log_uniform_sampling\",\n",
    "        type=str,\n",
    "        choices=[\"log_uniform_sampling\", \"time_uniform_sampling\", \"normal_sampling\"],\n",
    "        help=\"Choose a valid noise sampler from the list of choices\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--noise_weighting\",\n",
    "        default=\"edm_weighting\",\n",
    "        type=str,\n",
    "        choices=[\"edm_weighting\", \"likelihood_weighting\"],\n",
    "        help=\"Choose a valid weighting method from the list of choices\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--num_eval_noise_levels\",\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help=\"Set number of noise levels for evaluation during training\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--num_eval_cases_per_lvl\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"Set number of evaluation samples per noise level\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--min_eval_noise_lvl\",\n",
    "        default=1e-3,\n",
    "        type=float,\n",
    "        help=\"Minimum noise level during evaluation\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--max_eval_noise_lvl\",\n",
    "        default=50.0,\n",
    "        type=float,\n",
    "        help=\"Maximum noise level during evaluation\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--consistent_weight\",\n",
    "        default=0.0,\n",
    "        type=float,\n",
    "        help=\"Set weighting for some loss terms\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_trainer_options(parser: ArgumentParser):\n",
    "    \"\"\"Parser Arguments for the Trainer\"\"\"\n",
    "\n",
    "    group = parser.add_argument_group(\"trainer\")\n",
    "    # EMA ... Exponential Moving Average\n",
    "    group.add_argument(\n",
    "        \"--ema_decay\",\n",
    "        default=0.999,\n",
    "        type=float,  # Before: 0.999\n",
    "        help=\"Choose a decay rate for the EMA model parameters\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--peak_lr\",\n",
    "        default=1e-4,\n",
    "        type=float,  # 1e-4 # before: 1e-3\n",
    "        help=\"Choose a learning rate for the Adam optimizer\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--weight_decay\",\n",
    "        default=0.01,\n",
    "        type=float,  # 0.01 , 1e-5 # before: 0.01\n",
    "        help=\"Regularization strength for the optimizer\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_training_options(parser: ArgumentParser):\n",
    "    \"\"\"Parser arguments for the training loop\"\"\"\n",
    "\n",
    "    group = parser.add_argument_group(\"training\")\n",
    "    group.add_argument(\n",
    "        \"--num_train_steps\",\n",
    "        default=10_000,\n",
    "        type=int,\n",
    "        help=\"Choose number of training steps\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--metric_aggregation_steps\",\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help=\"trainer runs this number of steps until training metrics are aggregated\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--eval_every_steps\",\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help=\"Period at which an evaluation loop runs\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--num_batches_per_eval\",\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help=\"Number of steps until evaluation metrics are aggregated\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--run_sanity_eval_batch\",\n",
    "        default=True,\n",
    "        type=str_to_bool,\n",
    "        help=\"Sanity check to spot early mistakes or runtime issues\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--checkpoints\",\n",
    "        default=True,\n",
    "        type=str_to_bool,\n",
    "        help=\"Saves or Loads parameters from a checkpoint\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--save_every_n_steps\",\n",
    "        default=10000,\n",
    "        type=int,\n",
    "        help=\"Saves a checkpoint of the model and optimizer after every n steps\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--track_memory\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If True, memory tracer during training is activated else returns zeros.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_sampler_options(parser: ArgumentParser):\n",
    "    \"\"\"Parser arguments for the sampler\"\"\"\n",
    "\n",
    "    group = parser.add_argument_group(\"sampler\")\n",
    "    group.add_argument(\n",
    "        \"--time_step_scheduler\",\n",
    "        default=\"edm_noise_decay\",\n",
    "        type=str,\n",
    "        choices=[\"edm_noise_decay\", \"exponential_noise_decay\", \"uniform_time\"],\n",
    "        help=\"Choose a valid time step scheduler for solving an SDE\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--sampling_steps\",\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help=\"Define sampling steps for solving the SDE, min value should be 32\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--apply_denoise_at_end\",\n",
    "        default=True,\n",
    "        type=str_to_bool,\n",
    "        help=\"If True applies the denoise function another time to the terminal states\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--return_full_paths\",\n",
    "        default=False,\n",
    "        type=str_to_bool,\n",
    "        help=\"If True the output of .generate() and .denoise() will contain the complete sampling path\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--rho\", default=7, type=int, help=\"Set decay rate for the noise over time\"\n",
    "    )\n",
    "\n",
    "\n",
    "def add_sde_options(parser: ArgumentParser):\n",
    "    \"\"\"Parser arguments for the Euler Maruyama Method\"\"\"\n",
    "\n",
    "    group = parser.add_argument_group(\"sde\")\n",
    "    group.add_argument(\n",
    "        \"--integrator\",\n",
    "        default=\"EulerMaruyame\",\n",
    "        type=str,\n",
    "        help=\"Choose a valid SDE Solver\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--time_axis_pos\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Defines the index where the time axis should be placed\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--terminal_only\",\n",
    "        default=True,\n",
    "        type=str_to_bool,\n",
    "        help=\"If set to False returns the full path otherwise only the terminal state\",\n",
    "    )\n",
    "\n",
    "\n",
    "def add_evaluation_options(parser: ArgumentParser):\n",
    "    \"\"\"Parser arguments to compute Metrics for the inference pipeline\"\"\"\n",
    "    group = parser.add_argument_group(\"evaluation\")\n",
    "    group.add_argument(\n",
    "        \"--compute_metrics\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If True metrics like mean and std will be computed\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--monte_carlo_samples\",\n",
    "        default=100,\n",
    "        type=int,\n",
    "        help=\"Choose a number of monte carlo samples to compute statistical metrics\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--visualize\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If True an image of a single generated result will be stored\",\n",
    "    )\n",
    "    group.add_argument(\n",
    "        \"--save_gen_samples\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If True an npz file with generated and groundtruth samples will be stored\",\n",
    "    )\n",
    "\n",
    "# def train_args():\n",
    "#     \"\"\"Define the Parser for the training\"\"\"\n",
    "\n",
    "#     parser = ArgumentParser()\n",
    "#     add_base_options(parser)\n",
    "#     add_parallelization_options(parser)\n",
    "#     add_data_options(parser)\n",
    "#     add_model_options(parser)\n",
    "#     add_denoiser_options(parser)\n",
    "#     add_trainer_options(parser)\n",
    "#     add_training_options(parser)\n",
    "#     return parser #.parse_args()\n",
    "\n",
    "def _getenv_cast(name, default, caster):\n",
    "    \"\"\"Read env var NAME and cast; fall back to default on any problem.\"\"\"\n",
    "    val = os.getenv(name, None)\n",
    "    if val is None or val == \"\":\n",
    "        return default\n",
    "    try:\n",
    "        if caster is bool:  # reuse your str_to_bool\n",
    "            return str_to_bool(str(val))\n",
    "        if caster is tuple:  # reuse your parse_tuple\n",
    "            return parse_tuple(val)\n",
    "        return caster(val)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def train_args(argv=None):\n",
    "    parser = ArgumentParser()\n",
    "    add_base_options(parser); add_parallelization_options(parser)\n",
    "    add_data_options(parser); add_model_options(parser)\n",
    "    add_denoiser_options(parser); add_trainer_options(parser); add_training_options(parser)\n",
    "\n",
    "    # hardcoded preset for your HIT3D run\n",
    "    parser.set_defaults(\n",
    "        dataset=\"HIT3D\",\n",
    "        model_type=\"PreconditionedDenoiser3D\",\n",
    "        save_dir=\"runs/hit3d_128_lb1_lf2_4gpu\",\n",
    "        batch_size=2,\n",
    "        num_blocks=1,\n",
    "        num_channels=(8,16),\n",
    "        padding_method=\"circular\",\n",
    "        use_mixed_precision=True,\n",
    "        world_size=1,\n",
    "        num_train_steps=100000,\n",
    "        metric_aggregation_steps=1,\n",
    "        eval_every_steps=1,\n",
    "        num_batches_per_eval=1,\n",
    "    )\n",
    "    return parser.parse_args([] if argv is None else argv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11b8ed4e-4aab-4288-ae2a-03f9d6b14811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/oscar/data/gk/voommen/no_diffusion/hit_3d/GenCFD_lf2/colab'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53846154-5988-4892-a4b8-02817d4f62c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/oscar/data/gk/voommen/no_diffusion/hit_3d/GenCFD_lf2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"/oscar/data/gk/voommen/no_diffusion/hit_3d/GenCFD_lf2\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29eab46f-797e-4562-b478-0ba4b3046172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Used device: cuda\n",
      "root: GenCFD/data/HIT3D_128\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "trajectory.npy not found at GenCFD/data/HIT3D_128/trajectory.npy",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsed device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# cwd = \"/oscar/data/gk/voommen/no_diffusion/hit_3d/GenCFD_lf2/GenCFD/train/train_gencfd.py\" #os.getcwd()\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# if args.save_dir is None:\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#     raise ValueError(\"Save directory not specified in arguments!\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#         os.makedirs(savedir)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#         print(f\"Created a directory to store metrics and models: {savedir}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m train_dataloader, eval_dataloader, dataset, time_cond = \u001b[43mget_dataset_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_worker\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefetch_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Default DataLoader value\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtrain_len:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_dataloader.dataset))\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33meval_len:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(eval_dataloader.dataset))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/oscar/data/gk/voommen/no_diffusion/hit_3d/GenCFD_lf2/GenCFD/utils/dataloader_builder.py:154\u001b[39m, in \u001b[36mget_dataset_loader\u001b[39m\u001b[34m(args, name, batch_size, num_worker, prefetch_factor, split, split_ratio)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a training and evaluation dataloader or a single dataloader\"\"\"\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# is_time_dependent passes the bool time_cond and tells if the problem is time\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# dependent or not\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m dataset, time_cond = \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_time_dependent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m use_persistent_workers = num_worker > \u001b[32m0\u001b[39m\n\u001b[32m    156\u001b[39m prefetch = prefetch_factor \u001b[38;5;28;01mif\u001b[39;00m num_worker > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/oscar/data/gk/voommen/no_diffusion/hit_3d/GenCFD_lf2/GenCFD/utils/dataloader_builder.py:115\u001b[39m, in \u001b[36mget_dataset\u001b[39m\u001b[34m(name, is_time_dependent)\u001b[39m\n\u001b[32m    112\u001b[39m     time_cond = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mHIT3D\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     dataset = \u001b[43mHIT3D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or handle metadata as needed\u001b[39;00m\n\u001b[32m    116\u001b[39m     time_cond = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/oscar/data/gk/voommen/no_diffusion/hit_3d/GenCFD_lf2/GenCFD/dataloader/hit3d.py:85\u001b[39m, in \u001b[36mHIT3D.__init__\u001b[39m\u001b[34m(self, root, split, padding_method, normalize_inputs, lb, lf, bounds, dtype)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.traj_path = Path(traj_path)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.traj_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrajectory.npy not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.traj_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m stats_dir = Path(os.getenv(\u001b[33m\"\u001b[39m\u001b[33mHIT3D_STATS_PATH\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.root)))\n\u001b[32m     88\u001b[39m mean_path = stats_dir / \u001b[33m\"\u001b[39m\u001b[33mMEAN.npy\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: trajectory.npy not found at GenCFD/data/HIT3D_128/trajectory.npy"
     ]
    }
   ],
   "source": [
    "# get arguments for training\n",
    "args = train_args()\n",
    "\n",
    "# Instead of parse_args(), do:\n",
    "# args, _ = train_args().parse_known_args()\n",
    "\n",
    "args.world_size=1\n",
    "\n",
    "# Initialize distributed mode (if multi-GPU)\n",
    "if args.world_size > 1:\n",
    "    args, device = init_distributed_mode(args)\n",
    "else:\n",
    "    print(\" \")\n",
    "    print(f\"Used device: {device}\")\n",
    "\n",
    "# cwd = \"/oscar/data/gk/voommen/no_diffusion/hit_3d/GenCFD_lf2/GenCFD/train/train_gencfd.py\" #os.getcwd()\n",
    "# if args.save_dir is None:\n",
    "#     raise ValueError(\"Save directory not specified in arguments!\")\n",
    "# savedir = os.path.join(cwd, args.save_dir)\n",
    "# if not os.path.exists(savedir):\n",
    "#     if (args.world_size > 1 and args.local_rank == 0) or args.world_size == 1:\n",
    "#         os.makedirs(savedir)\n",
    "#         print(f\"Created a directory to store metrics and models: {savedir}\")\n",
    "\n",
    "train_dataloader, eval_dataloader, dataset, time_cond = get_dataset_loader(\n",
    "    args=args,\n",
    "    name=args.dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    num_worker=args.worker,\n",
    "    prefetch_factor=2,  # Default DataLoader value\n",
    "    split=True,\n",
    "    split_ratio=0.99,\n",
    ")\n",
    "\n",
    "print(\"train_len:\", len(train_dataloader.dataset))\n",
    "print(\"eval_len:\", len(eval_dataloader.dataset))\n",
    "\n",
    "if (args.world_size > 1 and args.local_rank == 0) or args.world_size == 1:\n",
    "    # Save parameters in a JSON File\n",
    "    save_json_file(\n",
    "        args=args,\n",
    "        time_cond=time_cond,\n",
    "        split_ratio=0.8,\n",
    "        out_shape=dataset.output_shape,  # output shape of the prediction\n",
    "        input_channel=dataset.input_channel,\n",
    "        output_channel=dataset.output_channel,\n",
    "        spatial_resolution=dataset.spatial_resolution,\n",
    "        device=device,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "denoising_model = create_denoiser(\n",
    "    args=args,\n",
    "    input_channels=dataset.input_channel,\n",
    "    out_channels=dataset.output_channel,\n",
    "    spatial_resolution=dataset.spatial_resolution,\n",
    "    time_cond=time_cond,\n",
    "    device=device,\n",
    "    dtype=args.dtype,\n",
    "    use_ddp_wrapper=True,\n",
    ")\n",
    "\n",
    "if (args.world_size > 1 and args.local_rank == 0) or args.world_size == 1:\n",
    "    # Print number of Parameters:\n",
    "    model_params = sum(\n",
    "        p.numel() for p in denoising_model.denoiser.parameters() if p.requires_grad\n",
    "    )\n",
    "    print(\" \")\n",
    "    print(f\"Total number of model parameters: {model_params}\")\n",
    "    print(\" \")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    denoising_model.denoiser.parameters(),\n",
    "    lr=args.peak_lr,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "\n",
    "trainer = training_loop.trainers.DenoisingTrainer(\n",
    "    model=denoising_model,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    ema_decay=args.ema_decay,\n",
    "    store_ema=True,  # Store ema model as well\n",
    "    track_memory=args.track_memory,\n",
    "    use_mixed_precision=args.use_mixed_precision,\n",
    "    is_compiled=args.compile,\n",
    "    world_size=args.world_size,\n",
    "    local_rank=args.local_rank,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c97f4-52f6-437a-8ec2-136805aebb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb98ab2d-0127-4aa6-9c45-dc90d6ede938",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'denoising_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33m/oscar/data/gk/voommen/no_diffusion/hit_3d/GenCFD_lf2/runs/hit3d_128_lb1_lf2_4gpu/checkpoints\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m latest_model_path = utils.get_latest_checkpoint(model_path)\n\u001b[32m      4\u001b[39m trained_state = train.TrainState.restore_from_checkpoint(\n\u001b[32m      5\u001b[39m     latest_model_path,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     model=\u001b[43mdenoising_model\u001b[49m.denoiser,\n\u001b[32m      7\u001b[39m     optimizer=trainer.optimizer,\n\u001b[32m      8\u001b[39m     is_compiled=trainer.is_compiled,\n\u001b[32m      9\u001b[39m     is_parallelized=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     10\u001b[39m     use_ema=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     11\u001b[39m     device=device,\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'denoising_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = \"/oscar/data/gk/voommen/no_diffusion/hit_3d/GenCFD_lf2/runs/hit3d_128_lb1_lf2_4gpu/checkpoints\"\n",
    "latest_model_path = utils.get_latest_checkpoint(model_path)\n",
    "\n",
    "trained_state = train.TrainState.restore_from_checkpoint(\n",
    "    latest_model_path,\n",
    "    model=denoising_model.denoiser,\n",
    "    optimizer=trainer.optimizer,\n",
    "    is_compiled=trainer.is_compiled,\n",
    "    is_parallelized=False,\n",
    "    use_ema=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f4c9f-a68d-4009-9245-7ccc757d1e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GenCFD)",
   "language": "python",
   "name": "gencfd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
