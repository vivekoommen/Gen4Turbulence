{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56354684-43ab-4869-871a-554b1b280283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR\n",
    "from tcunet import Unet3D\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchinfo import summary\n",
    "import torchprofile\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(23)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "DTYPE = torch.float32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 200\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e507cc25-d636-493d-a1b5-9590f8267893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your custom loss function here\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, Par):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.Par = Par\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true = (y_true - self.Par[\"out_shift\"])/self.Par[\"out_scale\"]\n",
    "        y_pred = (y_pred - self.Par[\"out_shift\"])/self.Par[\"out_scale\"]\n",
    "        loss = torch.norm(y_true-y_pred, p=2)/torch.norm(y_true, p=2)\n",
    "        return loss\n",
    "\n",
    "class YourDataset_train(Dataset):\n",
    "    def __init__(self, x, t, y, transform=None):\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_sample = self.x[idx]\n",
    "        t_sample = self.t[idx]\n",
    "        y_sample = self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x_sample, t_sample, y_sample = self.transform(x_sample, t_sample, y_sample)\n",
    "\n",
    "        return x_sample, t_sample, y_sample\n",
    "    \n",
    "class YourDataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_sample = self.x[idx]\n",
    "        y_sample = self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x_sample, y_sample = self.transform(x_sample, y_sample)\n",
    "\n",
    "        return x_sample, y_sample\n",
    "\n",
    "\n",
    "def preprocess_train(traj, Par):\n",
    "    nsamples = traj.shape[0]\n",
    "    nt = traj.shape[1]\n",
    "    temp = nt - Par['lb'] - Par['lf'] + 1\n",
    "    x_idx = np.arange(temp).reshape(-1,1)\n",
    "    x_idx = np.tile(x_idx, (1, Par['lf'])).reshape(-1,1)\n",
    "\n",
    "    x_idx_ls = []\n",
    "    for i in range(Par[\"lb\"]):\n",
    "        x_idx_ls.append(x_idx+i)\n",
    "    x_idx = np.concatenate(x_idx_ls, axis=1)\n",
    "\n",
    "    t_idx = np.arange(Par['lf']).reshape(1,-1)\n",
    "\n",
    "    t_idx = np.tile(t_idx, (temp,1)).reshape(-1,)\n",
    "\n",
    "    y_idx = np.arange(nt)\n",
    "    y_idx = sliding_window_view(y_idx[Par['lb']:], window_shape=Par['lf']).reshape(-1,)\n",
    "\n",
    "    print(f\"x_idx: {x_idx.shape}\")\n",
    "    print(f\"t_idx: {t_idx.shape}\")\n",
    "    print(f\"y_idx: {y_idx.shape}\")\n",
    "\n",
    "    return x_idx, t_idx, y_idx\n",
    "\n",
    "def preprocess(traj, Par):\n",
    "    nsamples = traj.shape[0]\n",
    "    nt = traj.shape[1]\n",
    "    temp = nt - Par['lb'] - Par['LF'] + 1\n",
    "    x_idx = np.arange(temp).reshape(-1,1)\n",
    "    # x_idx = np.tile(x_idx, (1, Par['LF'])).reshape(-1,1)\n",
    "\n",
    "    x_idx_ls = []\n",
    "    for i in range(Par[\"lb\"]):\n",
    "        x_idx_ls.append(x_idx+i)\n",
    "    x_idx = np.concatenate(x_idx_ls, axis=1)\n",
    "\n",
    "    t_idx = np.arange(Par['lf']).reshape(-1,)\n",
    "    # t_idx = np.tile(t_idx, (temp,1)).reshape(-1,)\n",
    "\n",
    "    y_idx = np.arange(nt)\n",
    "    y_idx = sliding_window_view(y_idx[Par['lb']:], window_shape=Par['LF'])#.reshape(-1,)\n",
    "\n",
    "    print(f\"x_idx: {x_idx.shape}\")\n",
    "    print(f\"t_idx: {t_idx.shape}\")\n",
    "    print(f\"y_idx: {y_idx.shape}\")\n",
    "\n",
    "    return x_idx, t_idx, y_idx\n",
    "\n",
    "\n",
    "def combined_scheduler(optimizer, total_epochs, warmup_epochs, last_epoch=-1):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            return 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / (total_epochs - warmup_epochs)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "def rollout(model, x,t,NT, Par, batch_size):\n",
    "    # x - [bs, lb, nf, nx,ny]\n",
    "    # t - [lf,]\n",
    "    # NT - length of target time-series\n",
    "\n",
    "    # print('NT: ', NT)\n",
    "\n",
    "    y_pred_ls = []\n",
    "\n",
    "    bs = batch_size\n",
    "    end= bs\n",
    "    while True:\n",
    "        start = end-bs\n",
    "        out_ls = []\n",
    "        \n",
    "        temp_x1 = x[start:end] #[BS, lb, nf, nx,ny]\n",
    "        out_ls = [temp_x1.to(device)]\n",
    "        traj = torch.cat(out_ls, dim=1)\n",
    "\n",
    "        while traj.shape[1] < NT:\n",
    "            # model.eval()\n",
    "            with torch.no_grad():\n",
    "                temp_x = torch.repeat_interleave(temp_x1, Par['lf'], dim=0) #[BS*lf, lb, nf, nx,ny]\n",
    "                temp_t = t.repeat(traj.shape[0]) #[BS*lf, ]\n",
    "                if True:\n",
    "                    out = model(temp_x.to(device), temp_t.to(device)).reshape(-1,Par['lf'], Par['nf'],Par['nx'],Par['ny'], Par['nz']) #[BS, lf, nf, nx,ny]\n",
    "                out_ls.append(out)\n",
    "                traj = torch.cat(out_ls, dim=1)\n",
    "                temp_x1 = traj[:,-Par['lb']:] #[BS, lb, nf, nx,ny]\n",
    "                \n",
    "        pred = torch.cat(out_ls, dim=1)[:, Par['lb']:NT] #[BS, lf, nf, nx, ny]\n",
    "        y_pred_ls.append(pred)\n",
    "\n",
    "        end = end+bs\n",
    "        if end-bs > x.shape[0]-1:\n",
    "            break\n",
    "\n",
    "    y_pred = torch.cat(y_pred_ls, dim=0)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def compute_tke_spectrum_timeseries_torch(u_all, lx, ly, lz, smooth=False, device='cpu'):\n",
    "    \"\"\"\n",
    "    Compute TKE spectrum over time without loops using PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    u_all : torch.Tensor\n",
    "        Tensor of shape [3, ntime, nx, ny, nz] with velocity components.\n",
    "    lx, ly, lz : float\n",
    "        Domain sizes.\n",
    "    smooth : bool\n",
    "        Apply optional smoothing.\n",
    "    device : str\n",
    "        Device to use.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    knyquist : float\n",
    "    wave_numbers : torch.Tensor [maxbin]\n",
    "    tke_spectrum : torch.Tensor [ntime, maxbin]\n",
    "    \"\"\"\n",
    "    u_all = torch.tensor(u_all, dtype=DTYPE, device=device) # [3, ntime, nx, ny, nz]\n",
    "    nf, ntime, nx, ny, nz = u_all.shape\n",
    "    assert nf == 3, \"First dimension must be 3 (velocity components)\"\n",
    "\n",
    "    ntot = nx * ny * nz\n",
    "\n",
    "    # FFT\n",
    "    uh_all = torch.fft.fftn(u_all, dim=(-3, -2, -1)) / ntot  # [3, ntime, nx, ny, nz]\n",
    "\n",
    "    # Compute energy: sum of squares of real and imaginary parts\n",
    "    energy = 0.5 * (uh_all.real**2 + uh_all.imag**2).sum(dim=0)  # [ntime, nx, ny, nz]\n",
    "\n",
    "    # Wavenumber grids\n",
    "    kx = torch.fft.fftfreq(nx, d=lx / nx, device=device)\n",
    "    ky = torch.fft.fftfreq(ny, d=ly / ny, device=device)\n",
    "    kz = torch.fft.fftfreq(nz, d=lz / nz, device=device)\n",
    "    KX, KY, KZ = torch.meshgrid(kx, ky, kz, indexing='ij')\n",
    "    K_mag = torch.sqrt(KX**2 + KY**2 + KZ**2)  # [nx, ny, nz]\n",
    "\n",
    "    # Flatten for binning\n",
    "    k_bins = torch.round(K_mag.flatten() / K_mag.max() * (nx // 2)).to(torch.int64)  # [npts]\n",
    "    max_bin = k_bins.max().item() + 1\n",
    "\n",
    "    wave_numbers = torch.arange(max_bin, device=device) * (2 * np.pi / ((lx + ly + lz)/3))\n",
    "\n",
    "    # Flatten energy: [ntime, npts]\n",
    "    energy_flat = energy.view(ntime, -1)  # [ntime, npts]\n",
    "\n",
    "    # Allocate spectrum\n",
    "    tke_spectrum = torch.zeros(ntime, max_bin, device=device)\n",
    "\n",
    "    # Vectorized binning using broadcasting + scatter_add\n",
    "    for k in range(max_bin):\n",
    "        mask = (k_bins == k)\n",
    "        if mask.any():\n",
    "            tke_spectrum[:, k] = energy_flat[:, mask].sum(dim=1)\n",
    "\n",
    "    if smooth:\n",
    "        kernel = torch.ones(5, device=device) / 5\n",
    "        kernel = kernel[None, None, :]\n",
    "        tke_spectrum = torch.nn.functional.conv1d(\n",
    "            tke_spectrum[:, None, :], kernel, padding=2\n",
    "        ).squeeze(1)\n",
    "\n",
    "    knyquist = (2 * np.pi / ((lx + ly + lz)/3)) * min(nx, ny, nz) / 2\n",
    "\n",
    "    return knyquist, wave_numbers.detach().cpu().numpy(), tke_spectrum.detach().cpu().numpy()  # [ntime, maxbin]\n",
    "\n",
    "\n",
    "def make_images(true, pred, epoch):\n",
    "    # T,P - bs, nf, nx, ny, nz\n",
    "    true = true.permute(0,2,1,3,4,5)\n",
    "    pred = pred.permute(0,2,1,3,4,5)\n",
    "\n",
    "    sample_id = 0\n",
    "    f_id = 0\n",
    "    t_id = 4\n",
    "    z_id = 64\n",
    "\n",
    "    CMAP = \"viridis\"\n",
    "    \n",
    "    knyquist, wave_numbers, spectrum_true = compute_tke_spectrum_timeseries_torch( true[sample_id, :3], 2*np.pi, 2*np.pi, 2*np.pi, device=device  )\n",
    "    knyquist, wave_numbers, spectrum_pred = compute_tke_spectrum_timeseries_torch( pred[sample_id, :3], 2*np.pi, 2*np.pi, 2*np.pi, device=device  )\n",
    "    \n",
    "\n",
    "    T = true[sample_id, f_id, t_id, :, :, z_id].detach().cpu().numpy()\n",
    "    P = pred[sample_id, f_id, t_id, :, :, z_id].detach().cpu().numpy()\n",
    "\n",
    "    VMIN = np.min(T)\n",
    "    VMAX = np.max(T)\n",
    "    # VMIN = -1\n",
    "    # VMAX = +1\n",
    "\n",
    "    fig, axes = plt.subplots(1,3, figsize=(18,5))\n",
    "    im = axes[0].imshow(T, cmap=CMAP, vmin=VMIN, vmax=VMAX)\n",
    "    axes[0].set_title(\"True\")\n",
    "    fig.colorbar(im, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    axes[1].imshow(P, cmap=CMAP, vmin=VMIN, vmax=VMAX)\n",
    "    axes[1].set_title(\"Pred\")\n",
    "    fig.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    axes[2].loglog(wave_numbers, spectrum_true[t_id], label=\"True\", color=\"black\", lw=2.5)\n",
    "    axes[2].loglog(wave_numbers, spectrum_pred[t_id], label=\"Pred\", color=\"blue\", lw=1.5)\n",
    "    axes[2].loglog(wave_numbers, wave_numbers**(-5/3), label=\"slope = -5/3\", color=\"magenta\", ls=\"dotted\")\n",
    "    axes[2].legend()\n",
    "    axes[2].set_xlabel(\"k\", fontsize = 18)\n",
    "    axes[2].set_ylabel(\"E(k)\", fontsize=18)\n",
    "\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    fig.suptitle(f\"Epoch: {epoch}\", fontsize=22, y=1.2)\n",
    "    plt.savefig(f\"images/{epoch}.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1395df8-3ac9-47ce-a08c-ed1c5103033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traj: (1, 200, 4, 128, 128, 128)\n",
      "Data Loading Time: 2.1s\n",
      "\n",
      "Train Dataset\n",
      "x_idx: (612, 4)\n",
      "t_idx: (612,)\n",
      "y_idx: (612,)\n",
      "\n",
      "Validation Dataset\n",
      "x_idx: (1, 4)\n",
      "t_idx: (4,)\n",
      "y_idx: (1, 36)\n",
      "\n",
      "Test Dataset\n",
      "x_idx: (1, 4)\n",
      "t_idx: (4,)\n",
      "y_idx: (1, 36)\n",
      "Data Preprocess Time: 0.0s\n",
      "MEAN: (1, 4, 1, 1, 1)\n",
      "STD: (1, 4, 1, 1, 1)\n",
      "MIN: (1, 4, 1, 1, 1)\n",
      "MAX: (1, 4, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "res = 128\n",
    "begin_time = time.time()\n",
    "\n",
    "traj = np.load(\"../data/data.npy\") #[nf, nt, nx, ny, nz]\n",
    "traj = traj.transpose(1,0,2,3,4)[4:]\n",
    "traj = np.expand_dims(traj, axis=0) #[B, nt, nf, nx, ny, nz]\n",
    "print(f\"traj: {traj.shape}\")\n",
    "print(f\"Data Loading Time: {time.time() - begin_time:.1f}s\")\n",
    "\n",
    "traj_train = traj[:, :160]\n",
    "traj_val   = traj[:, 160:]\n",
    "traj_test  = traj[:, 160:]\n",
    "\n",
    "Par = {}\n",
    "# Par['nt'] = 100 \n",
    "Par['nx'] = traj_train.shape[-3]\n",
    "Par['ny'] = traj_train.shape[-2]\n",
    "Par['nz'] = traj_train.shape[-1]\n",
    "Par['nf'] = traj_train.shape[2]\n",
    "Par['d_emb'] = 128\n",
    "\n",
    "Par['lb'] = 4\n",
    "Par['lf'] = 4\n",
    "Par['LF'] = 36 # To directly forecast 36 timesteps which corresponds to over 5 eddy turnover time scales\n",
    "Par['channels'] = Par['nf']*Par['lb']\n",
    "\n",
    "\n",
    "time_cond = np.linspace(0, 1, Par['lf'])\n",
    "if Par['lf']==1:\n",
    "    time_cond = np.linspace(0, 1, Par['lf']) + 1\n",
    "\n",
    "\n",
    "begin_time = time.time()\n",
    "print('\\nTrain Dataset')\n",
    "x_idx_train, t_idx_train, y_idx_train = preprocess_train(traj_train, Par)\n",
    "print('\\nValidation Dataset')\n",
    "x_idx_val, t_idx_val, y_idx_val  = preprocess(traj_val, Par)\n",
    "print('\\nTest Dataset')\n",
    "x_idx_test, t_idx_test, y_idx_test  = preprocess(traj_test, Par)\n",
    "print(f\"Data Preprocess Time: {time.time() - begin_time:.1f}s\")\n",
    "\n",
    "# sys.exit()\n",
    "\n",
    "t_min = np.min(time_cond)\n",
    "t_max = np.max(time_cond)\n",
    "if Par['lf']==1:\n",
    "    t_min=0\n",
    "    t_max=1\n",
    "\n",
    "MEAN = np.load('../data/MEAN.npy').reshape(1,-1,1,1,1)\n",
    "STD  = np.load('../data/STD.npy').reshape(1,-1,1,1,1)\n",
    "MIN  = np.load('../data/MIN.npy').reshape(1,-1,1,1,1)\n",
    "MAX  = np.load('../data/MAX.npy').reshape(1,-1,1,1,1)\n",
    "print(f\"MEAN: {MEAN.shape}\\nSTD: {STD.shape}\\nMIN: {MIN.shape}\\nMAX: {MAX.shape}\")\n",
    "\n",
    "Par['inp_shift'] = torch.tensor(MEAN, dtype=DTYPE, device=device)\n",
    "Par['inp_scale'] = torch.tensor(STD, dtype=DTYPE, device=device)\n",
    "Par['out_shift'] = torch.tensor(MEAN, dtype=DTYPE, device=device)\n",
    "Par['out_scale'] = torch.tensor(STD, dtype=DTYPE, device=device)\n",
    "Par['t_shift']   = torch.tensor(t_min, dtype=DTYPE, device=device)\n",
    "Par['t_scale']   = torch.tensor(t_max - t_min, dtype=DTYPE, device=device)\n",
    "\n",
    "\n",
    "with open('Par.pkl', 'wb') as f:\n",
    "    pickle.dump(Par, f)\n",
    "\n",
    "\n",
    "# Create custom datasets\n",
    "traj_train_tensor = torch.tensor(traj_train, dtype=DTYPE)\n",
    "traj_val_tensor = torch.tensor(traj_val, dtype=DTYPE)\n",
    "traj_test_tensor = torch.tensor(traj_test, dtype=DTYPE)\n",
    "time_cond_tensor = torch.tensor(time_cond, dtype=DTYPE)\n",
    "\n",
    "\n",
    "train_dataset = YourDataset_train(x_idx_train, t_idx_train, y_idx_train)\n",
    "val_dataset = YourDataset(x_idx_val, y_idx_val)\n",
    "test_dataset = YourDataset(x_idx_test, y_idx_test)\n",
    "\n",
    "\n",
    "# Define data loaders\n",
    "train_batch_size = 8 #100\n",
    "val_batch_size   = 1 #100\n",
    "test_batch_size  = 1 #100\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=val_batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e58624b-1ab5-4ba9-aa7c-1b02e164ae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "Unet3D                                        [1, 4, 128, 128, 128]     --\n",
      "├─Conv3d: 1-1                                 [1, 16, 128, 128, 128]    87,824\n",
      "├─Sequential: 1-2                             [1, 64]                   --\n",
      "│    └─SinusoidalPosEmb: 2-1                  [1, 16]                   --\n",
      "│    └─Linear: 2-2                            [1, 64]                   1,088\n",
      "│    └─GELU: 2-3                              [1, 64]                   --\n",
      "│    └─Linear: 2-4                            [1, 64]                   4,160\n",
      "├─ModuleList: 1-3                             --                        --\n",
      "│    └─ModuleList: 2-5                        --                        --\n",
      "│    │    └─ResnetBlock: 3-1                  [1, 16, 128, 128, 128]    16,000\n",
      "│    │    └─ResnetBlock: 3-2                  [1, 16, 128, 128, 128]    16,000\n",
      "│    │    └─Identity: 3-3                     [1, 16, 128, 128, 128]    --\n",
      "│    │    └─Sequential: 3-4                   [1, 16, 64, 64, 64]       2,064\n",
      "│    └─ModuleList: 2-6                        --                        --\n",
      "│    │    └─ResnetBlock: 3-5                  [1, 16, 64, 64, 64]       16,000\n",
      "│    │    └─ResnetBlock: 3-6                  [1, 16, 64, 64, 64]       16,000\n",
      "│    │    └─Identity: 3-7                     [1, 16, 64, 64, 64]       --\n",
      "│    │    └─Sequential: 3-8                   [1, 32, 32, 32, 32]       4,128\n",
      "│    └─ModuleList: 2-7                        --                        --\n",
      "│    │    └─ResnetBlock: 3-9                  [1, 32, 32, 32, 32]       59,648\n",
      "│    │    └─ResnetBlock: 3-10                 [1, 32, 32, 32, 32]       59,648\n",
      "│    │    └─Identity: 3-11                    [1, 32, 32, 32, 32]       --\n",
      "│    │    └─Sequential: 3-12                  [1, 64, 16, 16, 16]       16,448\n",
      "│    └─ModuleList: 2-8                        --                        --\n",
      "│    │    └─ResnetBlock: 3-13                 [1, 64, 16, 16, 16]       229,888\n",
      "│    │    └─ResnetBlock: 3-14                 [1, 64, 16, 16, 16]       229,888\n",
      "│    │    └─Identity: 3-15                    [1, 64, 16, 16, 16]       --\n",
      "│    │    └─Conv3d: 3-16                      [1, 128, 16, 16, 16]      221,312\n",
      "├─ResnetBlock: 1-4                            [1, 128, 16, 16, 16]      --\n",
      "│    └─Sequential: 2-9                        [1, 256]                  --\n",
      "│    │    └─SiLU: 3-17                        [1, 64]                   --\n",
      "│    │    └─Linear: 3-18                      [1, 256]                  16,640\n",
      "│    └─Block: 2-10                            [1, 128, 16, 16, 16]      --\n",
      "│    │    └─Conv3d: 3-19                      [1, 128, 16, 16, 16]      442,496\n",
      "│    │    └─GroupNorm: 3-20                   [1, 128, 16, 16, 16]      256\n",
      "│    │    └─SiLU: 3-21                        [1, 128, 16, 16, 16]      --\n",
      "│    └─Block: 2-11                            [1, 128, 16, 16, 16]      --\n",
      "│    │    └─Conv3d: 3-22                      [1, 128, 16, 16, 16]      442,496\n",
      "│    │    └─GroupNorm: 3-23                   [1, 128, 16, 16, 16]      256\n",
      "│    │    └─SiLU: 3-24                        [1, 128, 16, 16, 16]      --\n",
      "│    └─Identity: 2-12                         [1, 128, 16, 16, 16]      --\n",
      "├─Identity: 1-5                               [1, 128, 16, 16, 16]      --\n",
      "├─ResnetBlock: 1-6                            [1, 128, 16, 16, 16]      --\n",
      "│    └─Sequential: 2-13                       [1, 256]                  --\n",
      "│    │    └─SiLU: 3-25                        [1, 64]                   --\n",
      "│    │    └─Linear: 3-26                      [1, 256]                  16,640\n",
      "│    └─Block: 2-14                            [1, 128, 16, 16, 16]      --\n",
      "│    │    └─Conv3d: 3-27                      [1, 128, 16, 16, 16]      442,496\n",
      "│    │    └─GroupNorm: 3-28                   [1, 128, 16, 16, 16]      256\n",
      "│    │    └─SiLU: 3-29                        [1, 128, 16, 16, 16]      --\n",
      "│    └─Block: 2-15                            [1, 128, 16, 16, 16]      --\n",
      "│    │    └─Conv3d: 3-30                      [1, 128, 16, 16, 16]      442,496\n",
      "│    │    └─GroupNorm: 3-31                   [1, 128, 16, 16, 16]      256\n",
      "│    │    └─SiLU: 3-32                        [1, 128, 16, 16, 16]      --\n",
      "│    └─Identity: 2-16                         [1, 128, 16, 16, 16]      --\n",
      "├─ModuleList: 1-7                             --                        --\n",
      "│    └─ModuleList: 2-17                       --                        --\n",
      "│    │    └─ResnetBlock: 3-33                 [1, 128, 16, 16, 16]      1,148,032\n",
      "│    │    └─ResnetBlock: 3-34                 [1, 128, 16, 16, 16]      1,148,032\n",
      "│    │    └─Identity: 3-35                    [1, 128, 16, 16, 16]      --\n",
      "│    │    └─Sequential: 3-36                  [1, 64, 32, 32, 32]       221,248\n",
      "│    └─ModuleList: 2-18                       --                        --\n",
      "│    │    └─ResnetBlock: 3-37                 [1, 64, 32, 32, 32]       291,392\n",
      "│    │    └─ResnetBlock: 3-38                 [1, 64, 32, 32, 32]       291,392\n",
      "│    │    └─Identity: 3-39                    [1, 64, 32, 32, 32]       --\n",
      "│    │    └─Sequential: 3-40                  [1, 32, 64, 64, 64]       55,328\n",
      "│    └─ModuleList: 2-19                       --                        --\n",
      "│    │    └─ResnetBlock: 3-41                 [1, 32, 64, 64, 64]       75,040\n",
      "│    │    └─ResnetBlock: 3-42                 [1, 32, 64, 64, 64]       75,040\n",
      "│    │    └─Identity: 3-43                    [1, 32, 64, 64, 64]       --\n",
      "│    │    └─Sequential: 3-44                  [1, 16, 128, 128, 128]    13,840\n",
      "│    └─ModuleList: 2-20                       --                        --\n",
      "│    │    └─ResnetBlock: 3-45                 [1, 16, 128, 128, 128]    23,440\n",
      "│    │    └─ResnetBlock: 3-46                 [1, 16, 128, 128, 128]    23,440\n",
      "│    │    └─Identity: 3-47                    [1, 16, 128, 128, 128]    --\n",
      "│    │    └─Conv3d: 3-48                      [1, 16, 128, 128, 128]    6,928\n",
      "├─ResnetBlock: 1-8                            [1, 16, 128, 128, 128]    --\n",
      "│    └─Sequential: 2-21                       [1, 32]                   --\n",
      "│    │    └─SiLU: 3-49                        [1, 64]                   --\n",
      "│    │    └─Linear: 3-50                      [1, 32]                   2,080\n",
      "│    └─Block: 2-22                            [1, 16, 128, 128, 128]    --\n",
      "│    │    └─Conv3d: 3-51                      [1, 16, 128, 128, 128]    13,840\n",
      "│    │    └─GroupNorm: 3-52                   [1, 16, 128, 128, 128]    32\n",
      "│    │    └─SiLU: 3-53                        [1, 16, 128, 128, 128]    --\n",
      "│    └─Block: 2-23                            [1, 16, 128, 128, 128]    --\n",
      "│    │    └─Conv3d: 3-54                      [1, 16, 128, 128, 128]    6,928\n",
      "│    │    └─GroupNorm: 3-55                   [1, 16, 128, 128, 128]    32\n",
      "│    │    └─SiLU: 3-56                        [1, 16, 128, 128, 128]    --\n",
      "│    └─Conv3d: 2-24                           [1, 16, 128, 128, 128]    528\n",
      "├─Conv3d: 1-9                                 [1, 4, 128, 128, 128]     68\n",
      "===============================================================================================\n",
      "Total params: 6,181,044\n",
      "Trainable params: 6,181,044\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 528.23\n",
      "===============================================================================================\n",
      "Input size (MB): 134.22\n",
      "Forward/backward pass size (MB): 8445.25\n",
      "Params size (MB): 24.72\n",
      "Estimated Total Size (MB): 8604.19\n",
      "===============================================================================================\n",
      "GFLOPs: 1056.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::reshape\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pad\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::arange\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::exp\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::sin\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::cos\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gelu\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::silu\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::upsample_trilinear3d\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    }
   ],
   "source": [
    "model = Unet3D(dim=16, Par=Par, dim_mults=(1, 2, 4, 8), channels=Par['channels']).to(device).to(torch.float32)\n",
    "\n",
    "path_model = 'models/best_model.pt'\n",
    "model.load_state_dict(torch.load(path_model))\n",
    "\n",
    "dummy_x = torch.tensor(np.random.uniform(size=(1,Par['lb'], Par['nf'], Par['nx'], Par['ny'], Par['nz'])), dtype=DTYPE )\n",
    "dummy_t = torch.tensor(np.random.uniform(size=(1,)), dtype=DTYPE )\n",
    "dummy_input = (dummy_x.to(device), dummy_t.to(device))\n",
    "\n",
    "print(summary(model, input_size=((1,)+dummy_x.shape[1:], (1,)) ) )\n",
    "\n",
    "# # Profile the model\n",
    "flops = 2 * torchprofile.profile_macs(model, dummy_input)\n",
    "print(f\"GFLOPs: {(flops/10**9):.2f}\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = CustomLoss(Par)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b0be96-971e-47c4-b4e6-6d149d5243d2",
   "metadata": {},
   "source": [
    "# Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d9aff7-71dc-4233-9fb7-43bc26f23919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.02413\n",
      "Inference time: 0.08469\n",
      "Inference time: 0.08465\n",
      "Inference time: 0.08433\n",
      "Inference time: 0.08686\n",
      "Inference time: 0.08336\n",
      "Inference time: 0.08429\n",
      "Inference time: 0.08390\n",
      "Inference time: 0.08374\n",
      "Inference time: 0.08363\n",
      "Inference time: 0.08359\n",
      "Inference time: 0.08362\n",
      "Inference time: 0.08351\n",
      "Inference time: 0.08357\n",
      "Inference time: 0.08472\n",
      "\n",
      "mean: 0.08379430770874023\n"
     ]
    }
   ],
   "source": [
    "inference_time_ls = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i in range(15):\n",
    "    begin_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(dummy_x.to(device), dummy_t.to(device))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - begin_time\n",
    "    print(f\"Inference time: {inference_time:.5f}\")\n",
    "    inference_time_ls.append(inference_time)\n",
    "\n",
    "print()\n",
    "print(f\"mean: {np.mean(inference_time_ls[5:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d3767-57e1-4873-9118-3929571d37c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "450fc005-c949-4fb1-8d7b-9713620cde2f",
   "metadata": {},
   "source": [
    "# Peak VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daad6aeb-67c0-4f85-b52d-027108b4f803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak VRAM (allocated): 1.8959 GB\n",
      "Peak VRAM (reserved) : 9.5043 GB\n",
      "Config: batch=1, dtype= torch.float32 , device= cuda\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = False  # keep runs reproducible\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Warmup\n",
    "with torch.no_grad():\n",
    "    y_pred = model(dummy_x.to(device), dummy_t.to(device))\n",
    "\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats() \n",
    "with torch.no_grad():\n",
    "    y_pred = model(dummy_x.to(device), dummy_t.to(device))\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "# ---- Read peaks (bytes) and report in GB ----\n",
    "peak_alloc_GB   = torch.cuda.max_memory_allocated()  / 1e9\n",
    "peak_resvd_GB   = torch.cuda.max_memory_reserved()   / 1e9\n",
    "print(f\"Peak VRAM (allocated): {peak_alloc_GB:.4f} GB\")\n",
    "print(f\"Peak VRAM (reserved) : {peak_resvd_GB:.4f} GB\")\n",
    "print(\"Config: batch=1, dtype=\", DTYPE, \", device=\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3377d2d-d412-48b3-8563-004d6dd5cbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bb94e56-e27c-4267-97e4-a85a7b044e04",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7fc96cd-5cc6-4c08-8efb-c514719b3fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torch/utils/data/_utils/collate.py:183: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.3839e-01\n",
      "TEST_TRUE: (1, 4, 36, 128, 128, 128), DTYPE: float32\n",
      "TEST_PRED: (1, 4, 36, 128, 128, 128), DTYPE: float32\n"
     ]
    }
   ],
   "source": [
    "y_true_ls = []\n",
    "y_pred_ls = []\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for x_idx, y_idx in test_loader:\n",
    "        x = traj_test_tensor[0, x_idx]        #[BS, lb, nf, nx, ny]\n",
    "        t = time_cond_tensor[t_idx_test]      #[lf, ]\n",
    "        y_true = traj_test_tensor[0, y_idx]   #[BS,lf, nf, nx, ny]\n",
    "        y_pred = rollout(model, x,t,Par['lb']+Par['LF'], Par, test_batch_size)\n",
    "        # print(f\"y_true: {y_true.shape}\")\n",
    "        # print(f\"y_pred: {y_pred.shape}\")\n",
    "\n",
    "        loss   = criterion(y_pred, y_true.to(device))\n",
    "        test_loss += loss.item()\n",
    "        y_true_ls.append(y_true.detach().cpu().numpy())\n",
    "        y_pred_ls.append(y_pred.detach().cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4e}\")\n",
    "\n",
    "TEST_TRUE = np.concatenate(y_true_ls, axis=0).reshape(-1, Par['LF'], Par['nf'], Par['nx'], Par['ny'], Par['nz']).astype(np.float32).transpose(0,2,1,3,4,5)\n",
    "TEST_PRED = np.concatenate(y_pred_ls, axis=0).reshape(-1, Par['LF'], Par['nf'], Par['nx'], Par['ny'], Par['nz']).astype(np.float32).transpose(0,2,1,3,4,5)\n",
    "\n",
    "print(f\"TEST_TRUE: {TEST_TRUE.shape}, DTYPE: {TEST_TRUE.dtype}\")\n",
    "print(f\"TEST_PRED: {TEST_PRED.shape}, DTYPE: {TEST_PRED.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99917fd-1190-43c7-bf02-a2364117b835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "214257f1-8717-4f61-a33e-b5b830e50a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"TEST_TRUE.npy\", TEST_TRUE)\n",
    "np.save(\"TEST_PRED.npy\", TEST_PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47719d9c-4704-446e-9c39-ce9a5eeed951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
