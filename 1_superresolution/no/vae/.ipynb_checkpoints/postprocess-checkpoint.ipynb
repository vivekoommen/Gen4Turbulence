{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0dc682a-b9be-475c-b892-78d1a47fb1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "\n",
    "from model import *\n",
    "\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from matcho import Unet2D\n",
    "# from YourDataset import YourDataset  # Import your custom dataset here\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchinfo import summary\n",
    "import torchprofile\n",
    "\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(23)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "DTYPE = torch.float32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 200\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa50d0de-1b99-49c1-be70-01511674ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power(true, pred, inp):\n",
    "    BS, nt, nx, ny = true.shape\n",
    "    \n",
    "    # Compute the Fourier transforms and amplitude squared for both true and pred\n",
    "    fourier_true = torch.fft.fftn(true, dim=(-2, -1))\n",
    "    fourier_pred = torch.fft.fftn(pred, dim=(-2, -1))\n",
    "    fourier_inp  = torch.fft.fftn(inp , dim=(-2, -1))\n",
    "\n",
    "    # Get the squared amplitudes\n",
    "    amplitudes_true = torch.abs(fourier_true) #** 2\n",
    "    amplitudes_pred = torch.abs(fourier_pred) #** 2\n",
    "    amplitudes_inp = torch.abs(fourier_inp) #** 2\n",
    "\n",
    "    # Create the k-frequency grids\n",
    "    kfreq_y = torch.fft.fftfreq(ny) * ny\n",
    "    kfreq_x = torch.fft.fftfreq(nx) * nx\n",
    "    kfreq2D_x, kfreq2D_y = torch.meshgrid(kfreq_x, kfreq_y, indexing='ij')\n",
    "    \n",
    "    # Compute the wavenumber grid\n",
    "    knrm = torch.sqrt(kfreq2D_x ** 2 + kfreq2D_y ** 2).to(true.device)\n",
    "    \n",
    "    # Define the bins for the wavenumber\n",
    "    kbins = torch.arange(0.5, nx // 2 + 1, 1.0, device=true.device)\n",
    "    \n",
    "    # Digitize knrm to bin indices\n",
    "    knrm_flat = knrm.flatten()\n",
    "    bin_indices = torch.bucketize(knrm_flat, kbins)\n",
    "\n",
    "    # Reshape and flatten the amplitudes\n",
    "    amplitudes_true_flat = amplitudes_true.view(BS, nt, nx * ny)\n",
    "    amplitudes_pred_flat = amplitudes_pred.view(BS, nt, nx * ny)\n",
    "    amplitudes_inp_flat  = amplitudes_inp.view(BS, nt, nx * ny)\n",
    "\n",
    "    # Initialize Abins\n",
    "    Abins_true = torch.zeros((BS, nt, len(kbins) - 1), device=true.device)\n",
    "    Abins_pred = torch.zeros((BS, nt, len(kbins) - 1), device=pred.device)\n",
    "    Abins_inp  = torch.zeros((BS, nt, len(kbins) - 1), device= inp.device)\n",
    "\n",
    "    # Vectorized binning: sum up the values in each bin\n",
    "    for bin_idx in range(1, len(kbins)):\n",
    "        mask = (bin_indices == bin_idx).unsqueeze(0).unsqueeze(0)  # Create a mask for each bin\n",
    "        Abins_true[:, :, bin_idx - 1] = (amplitudes_true_flat * mask).sum(dim=-1) / mask.sum(dim=-1)\n",
    "        Abins_pred[:, :, bin_idx - 1] = (amplitudes_pred_flat * mask).sum(dim=-1) / mask.sum(dim=-1)\n",
    "        Abins_inp[:,  :, bin_idx - 1] = (amplitudes_inp_flat  * mask).sum(dim=-1) / mask.sum(dim=-1)\n",
    "\n",
    "    # Scale the binned amplitudes\n",
    "    scaling_factor = torch.pi * (kbins[1:] ** 2 - kbins[:-1] ** 2)\n",
    "    Abins_true *= scaling_factor\n",
    "    Abins_pred *= scaling_factor\n",
    "    Abins_inp  *= scaling_factor\n",
    "\n",
    "    return Abins_true, Abins_pred, Abins_inp\n",
    "\n",
    "def plot_power_spectrum(power_inp, power_true, power_pred, inp, true, pred, epoch, err):\n",
    "    f = 2\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(4*f, 1*f))\n",
    "    # t_ls = np.arange(power_true.shape[1])\n",
    "    # skip_t = 12\n",
    "    # time_ls = t_ls[::skip_t][1:]\n",
    "\n",
    "    sample_id=8\n",
    "    t_id = 0\n",
    "    for i in range(1):\n",
    "        x = torch.arange(true.shape[-2]//2)\n",
    "        axes[0].loglog(x, power_true[sample_id,t_id], label='true', c='black')\n",
    "        axes[0].loglog(x, power_inp[sample_id,t_id], label='NO', c='blue')\n",
    "        axes[0].loglog(x, power_pred[sample_id,t_id], label='adv. NO', c='red')\n",
    "        # axes[i].set_title(f\"t: {0}\")\n",
    "        axes[0].set_xlabel(r'$k$')\n",
    "        if i==0:\n",
    "            axes[0].legend()\n",
    "        if i==0:\n",
    "            axes[0].set_ylabel(r'$P(k)$')\n",
    "    \n",
    "\n",
    "    inp_sample = inp[sample_id, t_id]\n",
    "    true_sample = true[sample_id, t_id]\n",
    "    pred_sample = pred[sample_id, t_id]\n",
    "    vmin, vmax = true_sample.min(), true_sample.max()\n",
    "    im1 = axes[1].imshow(true_sample, vmin=vmin, vmax=vmax, cmap=CMAP)\n",
    "    axes[1].set_title(\"True\")\n",
    "    axes[1].set_xticks([])\n",
    "    axes[1].set_yticks([])\n",
    "\n",
    "    im = axes[2].imshow(inp_sample, vmin=vmin, vmax=vmax, cmap=CMAP)\n",
    "    axes[2].set_title(\"NO\")\n",
    "    axes[2].set_xticks([])\n",
    "    axes[2].set_yticks([])\n",
    "\n",
    "    im = axes[3].imshow(pred_sample, vmin=vmin, vmax=vmax, cmap=CMAP)\n",
    "    axes[3].set_title(\"NO+VAE\")\n",
    "    axes[3].set_xticks([])\n",
    "    axes[3].set_yticks([])\n",
    "    fig.colorbar(im1, ax=axes[3])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    fig.suptitle(f\"Epoch: {epoch}, MSE: {err:.2e}\", fontsize=22, y=1.2)\n",
    "    plt.savefig(f\"power_spectrum/{epoch}.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def error_metric(inp, pred,true, epoch, Par={}, is_plot=True):\n",
    "    #re-normalize\n",
    "    # true = true*Par['out_scale'] + Par['out_shift']\n",
    "    # true = true*Par['out_scale'] + Par['out_shift']\n",
    "\n",
    "    # inp = inp*Par['inp_scale'] + Par['inp_shift']\n",
    "    # inp = (inp - Par['out_shift'])/Par['out_scale']\n",
    "\n",
    "    power_inp, power_true, power_pred = compute_power(inp, true, pred)\n",
    "    err = torch.mean( (torch.log(power_true)-torch.log(power_pred) )**2 )\n",
    "    f_err = torch.norm(true-pred, p=2)/torch.norm(true, p=2)\n",
    "    ref_err = torch.norm(true-inp, p=2)/torch.norm(true, p=2)\n",
    "    if is_plot:\n",
    "        plot_power_spectrum(power_inp.detach().cpu().numpy(), power_true.detach().cpu().numpy(), power_pred.detach().cpu().numpy(), inp.detach().cpu().numpy(), true.detach().cpu().numpy(), pred.detach().cpu().numpy(), epoch, err)\n",
    "    return err, f_err, ref_err\n",
    "\n",
    "######################## VO input #############################\n",
    "\n",
    "# Custom Dataset\n",
    "class SuperResDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data):\n",
    "        self.input_data = torch.tensor(input_data, dtype=torch.float32)\n",
    "        self.target_data = torch.tensor(target_data, dtype=torch.float32)\n",
    "\n",
    "        # # ðŸ”¥ Ensure data is in (BS, 20, Nx, Ny) shape\n",
    "        # if self.input_data.dim() == 5 and self.input_data.shape[1] == 1:\n",
    "        #     self.input_data = self.input_data.squeeze(1)  # Remove singleton channel\n",
    "        #     self.target_data = self.target_data.squeeze(1)  # Remove singleton channel\n",
    "\n",
    "        print(f\"input_data : {self.input_data.shape}\")\n",
    "        print(f\"target_data: {self.target_data.shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.target_data[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Load Data from .npy Files\n",
    "def load_data():\n",
    "    train_inputs = np.load('../TRAIN_PRED.npy',allow_pickle=True)  # Shape: (BS, Nt, Nx, Ny)\n",
    "    B,T,X,Y = train_inputs.shape\n",
    "    train_inputs = train_inputs.reshape(-1,1,X,Y)\n",
    "\n",
    "    train_targets = np.load('../TRAIN_TRUE.npy',allow_pickle=True)  # Shape: (BS, Nt, Nx, Ny)\n",
    "    train_targets = train_targets.reshape(-1,1,X,Y)\n",
    "\n",
    "    val_inputs = np.load('../VAL_PRED.npy',allow_pickle=True)  # Shape: (BS, Nt, Nx, Ny)\n",
    "    val_inputs = val_inputs.reshape(-1,1,X,Y)\n",
    "\n",
    "    val_targets = np.load('../VAL_TRUE.npy',allow_pickle=True) \n",
    "    val_targets = val_targets.reshape(-1,1,X,Y)\n",
    "\n",
    "    test_inputs = np.load('../TEST_PRED.npy',allow_pickle=True)  # Shape: (BS, Nt, Nx, Ny)\n",
    "    test_inputs = test_inputs.reshape(-1,1,X,Y)\n",
    "\n",
    "    test_targets = np.load('../TEST_TRUE.npy',allow_pickle=True) \n",
    "    test_targets = test_targets.reshape(-1,1,X,Y)\n",
    "    \n",
    "    return train_inputs, train_targets, val_inputs, val_targets, test_inputs, test_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e377834-7349-4a82-b27a-eb2d122fa752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inputs : (3980, 1, 128, 256)\n",
      "train_targets: (3980, 1, 128, 256)\n",
      "val_inputs   : (480, 1, 128, 256)\n",
      "val_targets  : (480, 1, 128, 256)\n",
      "test_inputs  : (480, 1, 128, 256)\n",
      "test_targets : (480, 1, 128, 256)\n",
      "Train Dataset prep\n",
      "input_data : torch.Size([3980, 1, 128, 256])\n",
      "target_data: torch.Size([3980, 1, 128, 256])\n",
      "Val Dataset prep\n",
      "input_data : torch.Size([480, 1, 128, 256])\n",
      "target_data: torch.Size([480, 1, 128, 256])\n",
      "Test Dataset prep\n",
      "input_data : torch.Size([480, 1, 128, 256])\n",
      "target_data: torch.Size([480, 1, 128, 256])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset\n",
    "train_inputs, train_targets, val_inputs, val_targets, test_inputs, test_targets = load_data()\n",
    "\n",
    "print(f\"train_inputs : {train_inputs.shape}\")\n",
    "print(f\"train_targets: {train_targets.shape}\")\n",
    "print(f\"val_inputs   : {val_inputs.shape}\")\n",
    "print(f\"val_targets  : {val_targets.shape}\")\n",
    "print(f\"test_inputs  : {test_inputs.shape}\")\n",
    "print(f\"test_targets : {test_targets.shape}\")\n",
    "\n",
    "os.makedirs(\"Params\", exist_ok=True)\n",
    "os.makedirs(\"power_spectrum\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 20\n",
    "\n",
    "print(f\"Train Dataset prep\")\n",
    "train_dataset = SuperResDataset(train_inputs, train_targets)\n",
    "print(f\"Val Dataset prep\")\n",
    "val_dataset = SuperResDataset(val_inputs, val_targets)\n",
    "print(f\"Test Dataset prep\")\n",
    "test_dataset = SuperResDataset(test_inputs, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "656e6e1e-3962-465c-af9f-5d7452d56dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "VQVAE                                                   [1, 1, 128, 256]          --\n",
      "â”œâ”€Encoder: 1-1                                          [1, 21, 64, 128]          --\n",
      "â”‚    â””â”€Conv2d: 2-1                                      [1, 21, 64, 128]          357\n",
      "â”‚    â””â”€Sequential: 2-2                                  [1, 21, 64, 128]          --\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-1                        [1, 21, 64, 128]          8,044\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-2                        [1, 21, 64, 128]          8,044\n",
      "â”‚    â””â”€Conv2d: 2-3                                      [1, 42, 32, 64]           14,154\n",
      "â”‚    â””â”€Sequential: 2-4                                  [1, 42, 32, 64]           --\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-3                        [1, 42, 32, 64]           32,048\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-4                        [1, 42, 32, 64]           32,048\n",
      "â”‚    â””â”€Conv2d: 2-5                                      [1, 84, 16, 32]           56,532\n",
      "â”‚    â””â”€Sequential: 2-6                                  [1, 84, 16, 32]           --\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-5                        [1, 84, 16, 32]           128,105\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-6                        [1, 84, 16, 32]           128,105\n",
      "â”‚    â””â”€SelfAttention: 2-7                               [1, 84, 16, 32]           1\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-7                                 [1, 10, 16, 32]           850\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-8                                 [1, 10, 16, 32]           850\n",
      "â”‚    â”‚    â””â”€Softmax: 3-9                                [1, 512, 512]             --\n",
      "â”‚    â”‚    â””â”€Conv2d: 3-10                                [1, 84, 16, 32]           7,140\n",
      "â”‚    â””â”€TransformerBottleneck: 2-8                       [1, 84, 16, 32]           --\n",
      "â”‚    â”‚    â””â”€TransformerEncoder: 3-11                    [1, 512, 84]              750,184\n",
      "â”‚    â””â”€Conv2d: 2-9                                      [1, 64, 16, 32]           48,448\n",
      "â”œâ”€VectorQuantizer: 1-2                                  [1, 64, 16, 32]           16,384\n",
      "â”œâ”€Decoder: 1-3                                          [1, 1, 128, 256]          --\n",
      "â”‚    â””â”€Conv2d: 2-10                                     [1, 84, 16, 32]           48,468\n",
      "â”‚    â””â”€Sequential: 2-11                                 [1, 84, 16, 32]           --\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-12                       [1, 84, 16, 32]           128,105\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-13                       [1, 84, 16, 32]           128,105\n",
      "â”‚    â””â”€TransformerBottleneck: 2-12                      [1, 84, 16, 32]           --\n",
      "â”‚    â”‚    â””â”€TransformerEncoder: 3-14                    [1, 512, 84]              375,092\n",
      "â”‚    â””â”€ConvTranspose2d: 2-13                            [1, 42, 32, 64]           56,490\n",
      "â”‚    â””â”€Sequential: 2-14                                 [1, 42, 32, 64]           --\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-15                       [1, 42, 32, 64]           32,048\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-16                       [1, 42, 32, 64]           32,048\n",
      "â”‚    â””â”€Conv2d: 2-15                                     [1, 42, 32, 64]           31,794\n",
      "â”‚    â””â”€ConvTranspose2d: 2-16                            [1, 21, 64, 128]          14,133\n",
      "â”‚    â””â”€Sequential: 2-17                                 [1, 21, 64, 128]          --\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-17                       [1, 21, 64, 128]          8,044\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-18                       [1, 21, 64, 128]          8,044\n",
      "â”‚    â””â”€Conv2d: 2-18                                     [1, 21, 64, 128]          7,959\n",
      "â”‚    â””â”€ConvTranspose2d: 2-19                            [1, 21, 128, 256]         7,077\n",
      "â”‚    â””â”€Sequential: 2-20                                 [1, 21, 128, 256]         --\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-19                       [1, 21, 128, 256]         8,044\n",
      "â”‚    â”‚    â””â”€ResidualBlockCA: 3-20                       [1, 21, 128, 256]         8,044\n",
      "â”‚    â””â”€Conv2d: 2-21                                     [1, 1, 128, 256]          190\n",
      "=========================================================================================================\n",
      "Total params: 2,124,979\n",
      "Trainable params: 2,124,979\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 2.02\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.13\n",
      "Forward/backward pass size (MB): 54.63\n",
      "Params size (MB): 3.93\n",
      "Estimated Total Size (MB): 58.69\n",
      "=========================================================================================================\n",
      "FLOPs: 5.26015e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unflatten\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scaled_dot_product_attention\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pow\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::argmin\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::one_hot\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::broadcast_tensors\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/oscar/home/voommen/apps/torch_env/lib64/python3.9/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::mse_loss\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    }
   ],
   "source": [
    "model = VQVAE(in_channels=1, hidden_channels=21, embedding_dim=64, num_embeddings=256, commitment_cost=0.25)\n",
    "path_model = f'Params/best_model.pt'\n",
    "model.load_state_dict(torch.load(path_model))\n",
    "\n",
    "\n",
    "print(summary(model, input_size=(1,)+train_inputs.shape[1:] ) )\n",
    "\n",
    "# Adjust the dimensions as per your model's input size\n",
    "dummy_x = torch.tensor(train_inputs[0:1], dtype=DTYPE, device=device)\n",
    "dummy_input = dummy_x\n",
    "\n",
    "# Profile the model\n",
    "model.eval()\n",
    "flops = 2 * torchprofile.profile_macs(model, dummy_input)\n",
    "print(f\"FLOPs: {flops:.5e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920fee6-b1c9-4a0c-a07c-723009e32033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439df004-3259-4b7c-989a-b32b9a8ee22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3dac9a0-0f02-4711-b8ab-9605ebcec264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matcho import Unet2D\n",
    "\n",
    "with open('../Par.pkl', 'rb') as f:\n",
    "    Par_no = pickle.load(f)\n",
    "\n",
    "no = Unet2D(dim=16, Par=Par_no, dim_mults=(1, 2, 4, 8)).to(device).to(torch.float32)\n",
    "path_model = '../models/best_model.pt'\n",
    "no.load_state_dict(torch.load(path_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c7346-2ee8-45ea-9bdd-0deb2f06c2dd",
   "metadata": {},
   "source": [
    "# Inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d61313-b68f-45c1-b13a-652a58860ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c876dd-6d80-4947-93be-b1364f67e542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.16827\n",
      "Inference time: 0.01427\n",
      "Inference time: 0.01396\n",
      "Inference time: 0.01394\n",
      "Inference time: 0.01398\n",
      "Inference time: 0.01403\n",
      "Inference time: 0.01393\n",
      "Inference time: 0.01396\n",
      "Inference time: 0.01398\n",
      "Inference time: 0.01402\n",
      "Inference time: 0.01392\n",
      "Inference time: 0.01398\n",
      "Inference time: 0.01391\n",
      "Inference time: 0.01391\n",
      "Inference time: 0.01396\n",
      "\n",
      "mean: 0.013959908485412597\n"
     ]
    }
   ],
   "source": [
    "inference_time_ls = []\n",
    "\n",
    "inp_x = torch.rand(size=(1,2,128,256), dtype=DTYPE, device=device)\n",
    "inp_t = torch.rand(size=(1,), dtype=DTYPE, device=device)\n",
    "\n",
    "no.eval()\n",
    "model.eval()\n",
    "\n",
    "for i in range(15):\n",
    "    begin_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        no_pred = no(inp_x, inp_t)\n",
    "        no_vae  = model(no_pred)\n",
    "        # for x, y_true in test_data_loader:\n",
    "        #     y_pred = model(x.to(device))\n",
    "        #     break\n",
    "\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - begin_time\n",
    "    print(f\"Inference time: {inference_time:.5f}\")\n",
    "    inference_time_ls.append(inference_time)\n",
    "\n",
    "print()\n",
    "print(f\"mean: {np.mean(inference_time_ls[5:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d5685-a7c2-4840-a002-e2d247aaeba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96568027-63c9-4030-ae9f-396aef2c05de",
   "metadata": {},
   "source": [
    "# PeakVRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aad39cb-8b2d-4340-90c7-345f51676200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak VRAM (allocated): 0.2982 GB\n",
      "Peak VRAM (reserved) : 0.4404 GB\n",
      "Config: batch=1, dtype= torch.float32 , device= cuda\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = False  # keep runs reproducible\n",
    "\n",
    "no.eval()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Warmup\n",
    "with torch.no_grad():\n",
    "    no_pred = no(inp_x, inp_t)\n",
    "    no_vae  = model(no_pred)\n",
    "\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats() \n",
    "with torch.no_grad():\n",
    "    no_pred = no(inp_x, inp_t)\n",
    "    no_vae  = model(no_pred)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "# ---- Read peaks (bytes) and report in GB ----\n",
    "peak_alloc_GB   = torch.cuda.max_memory_allocated()  / 1e9\n",
    "peak_resvd_GB   = torch.cuda.max_memory_reserved()   / 1e9\n",
    "print(f\"Peak VRAM (allocated): {peak_alloc_GB:.4f} GB\")\n",
    "print(f\"Peak VRAM (reserved) : {peak_resvd_GB:.4f} GB\")\n",
    "print(\"Config: batch=1, dtype=\", DTYPE, \", device=\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53aaf86-af73-4bd9-879c-34139882ee96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40bb1c12-5720-4eeb-a09f-24fe511d3252",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9795e813-8b00-419a-8164-600951135b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Val Loss: 2.1493e-03, spec err: 2.7974e-01, field err: 1.4656e-01, ref err: 1.2629e-01\n",
      "VAL_TRUE: (96, 5, 128, 256), DTYPE: float32\n",
      "VAL_PRED: (96, 5, 128, 256), DTYPE: float32\n",
      " Test Loss: 2.2392e-03, spec err: 2.8998e-01, field err: 1.4820e-01, ref err: 1.2915e-01\n",
      "TEST_TRUE: (96, 5, 128, 256), DTYPE: float32\n",
      "TEST_PRED: (96, 5, 128, 256), DTYPE: float32\n"
     ]
    }
   ],
   "source": [
    "recon_criterion = nn.MSELoss()\n",
    "\n",
    "y_true_ls = []\n",
    "y_pred_ls = []\n",
    "\n",
    "# Validation Step\n",
    "model.eval()\n",
    "total_val_loss = 0\n",
    "spec_err = 0.0\n",
    "field_err = 0.0\n",
    "ref_err = 0.0\n",
    "plot_flag = False\n",
    "with torch.no_grad():\n",
    "    for x_lr, x_hr in val_loader:\n",
    "        x_lr, x_hr = x_lr.to(device), x_hr.to(device)\n",
    "        outputs, vq_loss = model(x_lr)\n",
    "        recon_loss = recon_criterion(outputs, x_hr)\n",
    "        loss = recon_loss + vq_loss\n",
    "        \n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        s_err, f_err, r_err = error_metric(x_lr, outputs, x_hr, 0, is_plot=plot_flag)\n",
    "        spec_err += s_err.item()\n",
    "        field_err += f_err.item()\n",
    "        ref_err += r_err.item()\n",
    "        plot_flag = False\n",
    "\n",
    "        y_true_ls.append(x_hr.detach().cpu().numpy())\n",
    "        y_pred_ls.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "avg_val_loss = total_val_loss / len(val_loader)\n",
    "spec_err /= len(val_loader)\n",
    "field_err /= len(val_loader)\n",
    "ref_err /= len(val_loader)\n",
    "print(f\" Val Loss: {avg_val_loss:.4e}, spec err: {spec_err:.4e}, field err: {field_err:.4e}, ref err: {ref_err:.4e}\" )\n",
    "\n",
    "VAL_TRUE = np.concatenate(y_true_ls, axis=0).reshape(-1, 5, 128, 256).astype(np.float32)\n",
    "VAL_PRED = np.concatenate(y_pred_ls, axis=0).reshape(-1, 5, 128, 256).astype(np.float32)\n",
    "\n",
    "print(f\"VAL_TRUE: {VAL_TRUE.shape}, DTYPE: {VAL_TRUE.dtype}\")\n",
    "print(f\"VAL_PRED: {VAL_PRED.shape}, DTYPE: {VAL_PRED.dtype}\")\n",
    "\n",
    "\n",
    "\n",
    "y_true_ls = []\n",
    "y_pred_ls = []\n",
    "\n",
    "# Test Step\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "spec_err = 0.0\n",
    "field_err = 0.0\n",
    "ref_err = 0.0\n",
    "plot_flag = False\n",
    "with torch.no_grad():\n",
    "    for x_lr, x_hr in test_loader:\n",
    "        x_lr, x_hr = x_lr.to(device), x_hr.to(device)\n",
    "        outputs, vq_loss = model(x_lr)\n",
    "        recon_loss = recon_criterion(outputs, x_hr)\n",
    "        loss = recon_loss + vq_loss\n",
    "        \n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        s_err, f_err, r_err = error_metric(x_lr, outputs, x_hr, 0, is_plot=plot_flag)\n",
    "        spec_err += s_err.item()\n",
    "        field_err += f_err.item()\n",
    "        ref_err += r_err.item()\n",
    "        plot_flag = False\n",
    "\n",
    "        y_true_ls.append(x_hr.detach().cpu().numpy())\n",
    "        y_pred_ls.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_loader)\n",
    "spec_err /= len(test_loader)\n",
    "field_err /= len(test_loader)\n",
    "ref_err /= len(test_loader)\n",
    "print(f\" Test Loss: {avg_test_loss:.4e}, spec err: {spec_err:.4e}, field err: {field_err:.4e}, ref err: {ref_err:.4e}\" )\n",
    "\n",
    "TEST_TRUE = np.concatenate(y_true_ls, axis=0).reshape(-1, 5, 128, 256).astype(np.float32)\n",
    "TEST_PRED = np.concatenate(y_pred_ls, axis=0).reshape(-1, 5, 128, 256).astype(np.float32)\n",
    "\n",
    "print(f\"TEST_TRUE: {TEST_TRUE.shape}, DTYPE: {TEST_TRUE.dtype}\")\n",
    "print(f\"TEST_PRED: {TEST_PRED.shape}, DTYPE: {TEST_PRED.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5495e4b1-65b4-4480-8403-c1605a452c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"TEST_TRUE.npy\", TEST_TRUE)\n",
    "np.save(\"TEST_PRED.npy\", TEST_PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49739639-95f9-47cd-a07b-893794443bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
